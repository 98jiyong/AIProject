{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8rzQMJLOpZx-"
   },
   "source": [
    "## 챗봇 예제(역할 부여)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "NsBvwRX8pO_h"
   },
   "outputs": [],
   "source": [
    "# 사전 설치 : pip install ollama\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "import ollama\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.retrievers import WikipediaRetriever\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "dPGqP-0mpgmF"
   },
   "outputs": [],
   "source": [
    "# 챗봇의 기본적 질문, 답변 역할\n",
    "def ask_gemma(question):\n",
    "    # ollama를 사용하여 모델로부터 응답 생성\n",
    "    chatbot_role = \"You are a helpful assistant.\"\n",
    "    response = ollama.chat(model='gemma2', messages=[\n",
    "        {\"role\": \"system\", \"content\": chatbot_role},  # 챗봇의 기본 역할 부여\n",
    "        {\"role\": \"user\", \"content\": question}, # 질문\n",
    "    ])\n",
    "\n",
    "    return response['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ReMa2JuJpitO"
   },
   "outputs": [
    {
     "ename": "ResponseError",
     "evalue": "model requires more system memory (7.7 GiB) than is available (4.5 GiB)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResponseError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m세계에서 가장 높은 산을 알려줘.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mask_gemma\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "Cell \u001b[1;32mIn[6], line 5\u001b[0m, in \u001b[0;36mask_gemma\u001b[1;34m(question)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mask_gemma\u001b[39m(question):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# ollama를 사용하여 모델로부터 응답 생성\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     chatbot_role \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are a helpful assistant.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 5\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mollama\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgemma2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mchatbot_role\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 챗봇의 기본 역할 부여\u001b[39;49;00m\n\u001b[0;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# 질문\u001b[39;49;00m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\AiProject\\.venv\\lib\\site-packages\\ollama\\_client.py:332\u001b[0m, in \u001b[0;36mClient.chat\u001b[1;34m(self, model, messages, tools, stream, format, options, keep_alive)\u001b[0m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchat\u001b[39m(\n\u001b[0;32m    289\u001b[0m   \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    290\u001b[0m   model: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    297\u001b[0m   keep_alive: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    298\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[ChatResponse, Iterator[ChatResponse]]:\n\u001b[0;32m    299\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    300\u001b[0m \u001b[38;5;124;03m  Create a chat response using the requested model.\u001b[39;00m\n\u001b[0;32m    301\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;124;03m  Returns `ChatResponse` if `stream` is `False`, otherwise returns a `ChatResponse` generator.\u001b[39;00m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 332\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[43m    \u001b[49m\u001b[43mChatResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    334\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/api/chat\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatRequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    337\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    338\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_copy_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    339\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtool\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtool\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_copy_tools\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtools\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    340\u001b[0m \u001b[43m      \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    341\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    342\u001b[0m \u001b[43m      \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    343\u001b[0m \u001b[43m      \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_alive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    344\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_dump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexclude_none\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    345\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    346\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\AiProject\\.venv\\lib\\site-packages\\ollama\\_client.py:177\u001b[0m, in \u001b[0;36mClient._request\u001b[1;34m(self, cls, stream, *args, **kwargs)\u001b[0m\n\u001b[0;32m    173\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpart)\n\u001b[0;32m    175\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[1;32m--> 177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request_raw(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\u001b[38;5;241m.\u001b[39mjson())\n",
      "File \u001b[1;32mc:\\AiProject\\.venv\\lib\\site-packages\\ollama\\_client.py:122\u001b[0m, in \u001b[0;36mClient._request_raw\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    120\u001b[0m   r\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mHTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 122\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m ResponseError(e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mtext, e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "\u001b[1;31mResponseError\u001b[0m: model requires more system memory (7.7 GiB) than is available (4.5 GiB)"
     ]
    }
   ],
   "source": [
    "question = \"세계에서 가장 높은 산을 알려줘.\"\n",
    "response = ask_gemma(question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DV_ws7zMpkPN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R-cxqIfq5t-w"
   },
   "source": [
    "## 챗봇 예제(Gradio 사용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "rbN6safQ5w-s"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\AiProject\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# 사전 설치 : pip install gradio\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.schema import HumanMessage, AIMessage\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "jY0UmlOE50Hk"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\human-06\\AppData\\Local\\Temp\\ipykernel_10352\\3441953186.py:2: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import ChatOllama``.\n",
      "  model = ChatOllama(model=\"gemma2\", temperature=0.7, verbose=False)\n"
     ]
    }
   ],
   "source": [
    "# ChatOllama 모델 초기화\n",
    "model = ChatOllama(model=\"gemma2\", temperature=0.7, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "2zJaYFBo5507"
   },
   "outputs": [],
   "source": [
    "# 채팅 기록을 포함하여 응답을 생성하는 함수\n",
    "def chat(message, history):\n",
    "    # 이전 대화 기록을 ChatOllama 형식으로 변환\n",
    "    chat_history = []\n",
    "    for human, ai in history:\n",
    "        chat_history.append(HumanMessage(content=human))\n",
    "        chat_history.append(AIMessage(content=ai))\n",
    "\n",
    "    # 현재 메시지 추가\n",
    "    chat_history.append(HumanMessage(content=message))\n",
    "\n",
    "    # 모델을 사용하여 응답 생성\n",
    "    response = model.invoke(chat_history)\n",
    "\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "SdG5wA2g559M"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\AiProject\\.venv\\lib\\site-packages\\gradio\\components\\chatbot.py:248: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Gradio 인터페이스 설정\n",
    "demo = gr.ChatInterface(\n",
    "    fn=chat,\n",
    "    examples=[\n",
    "        \"안녕하세요!\",\n",
    "        \"인공지능에 대해 설명해주세요.\",\n",
    "        \"파이썬의 장점은 무엇인가요?\"\n",
    "    ],\n",
    "    title=\"AI 챗봇\",\n",
    "    description=\"질문을 입력하면 AI가 답변합니다.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "K61bFH905-v8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 서버 실행\n",
    "demo.launch(server_port=7861, server_name=\"0.0.0.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "cQnckD8r6BC-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7861\n"
     ]
    }
   ],
   "source": [
    "demo.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FCf5JjnI7Zpo"
   },
   "source": [
    "## 챗봇 예제(Gradio + csv 사용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "3oLSrKNtCzzJ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.schema import HumanMessage, AIMessage\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "bKpeX2zUDBX6"
   },
   "outputs": [],
   "source": [
    "# CSV 파일 로드\n",
    "df = pd.read_csv(\"./dataset/indata_kor.csv\", encoding='CP949')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "v5u024T1DQLq"
   },
   "outputs": [],
   "source": [
    "# 텍스트 분할\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "texts = text_splitter.split_text(\"\\n\".join(df.to_string()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "BLBU5vvtDbND"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\human-06\\AppData\\Local\\Temp\\ipykernel_10352\\2819514520.py:3: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\AiProject\\.venv\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\AiProject\\.venv\\lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\human-06\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "# 임베딩 모델 초기화\n",
    "# embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/distiluse-base-multilingual-cased-v2\")\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "KjdIcm9BDdh6"
   },
   "outputs": [],
   "source": [
    "# 벡터 데이터베이스 생성\n",
    "vectorstore = FAISS.from_texts(texts, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "yqqXwJ_mDfaY"
   },
   "outputs": [],
   "source": [
    "# ChatOllama 모델 초기화\n",
    "llm = ChatOllama(model=\"gemma2\", tempeature=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "YB0eyNjRDhP8"
   },
   "outputs": [],
   "source": [
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm,\n",
    "    vectorstore.as_retriever(search_kwargs={\"k\":1}),\n",
    "    return_source_documents=True,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "eCz9P2IxDi6j"
   },
   "outputs": [],
   "source": [
    "# 채팅 함수 정의\n",
    "def chat(message, history):\n",
    "    # 이전 대화 기록을 ConversationalRetrievalChain 형식으로 변환\n",
    "    chat_history = [(human, ai) for human, ai in history]\n",
    "\n",
    "    # 모델을 사용하여 응답 생성\n",
    "    response = qa_chain({\"question\": message, \"chat_history\": chat_history})\n",
    "\n",
    "    # 소스 문서 정보 추출\n",
    "    sources = set([doc.metadata.get('source', 'Unknown') for doc in response['source_documents']])\n",
    "    source_info = f\"\\n\\n참고 출처: {', '.join(sources)}\" if sources else \"\"\n",
    "\n",
    "    return response['answer'] + source_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "tm2rFQZQDl4i"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\AiProject\\.venv\\lib\\site-packages\\gradio\\components\\chatbot.py:248: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Gradio 인터페이스 설정\n",
    "demo = gr.ChatInterface(\n",
    "    fn=chat,\n",
    "    examples=[\n",
    "        \"한국폴리텍대학 스마트금융과 면접시에는 어떤걸 준비하고 가면 될까요?\",\n",
    "        \"스마트금융과에 대해 설명해주세요\",\n",
    "        \"한국폴리텍대한 추천할만한 학과 하나를 소개해주세요.\"\n",
    "    ],\n",
    "    title=\"대학 정보 AI 챗봇\",\n",
    "    description=\"스마트금융과에 대한 질문을 입력하면 AI가 CSV데이터를 참고하여 한글로 답변합니다.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "T4xwawRwDn6i"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\human-06\\AppData\\Local\\Temp\\ipykernel_10352\\4160252103.py:7: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = qa_chain({\"question\": message, \"chat_history\": chat_history})\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\gradio\\queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\gradio\\blocks.py\", line 2047, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\gradio\\blocks.py\", line 1592, in call_function\n",
      "    prediction = await fn(*processed_input)\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\gradio\\utils.py\", line 836, in async_wrapper\n",
      "    response = await f(*args, **kwargs)\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\gradio\\chat_interface.py\", line 618, in _submit_fn\n",
      "    response = await anyio.to_thread.run_sync(\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2505, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 1005, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"C:\\Users\\human-06\\AppData\\Local\\Temp\\ipykernel_10352\\4160252103.py\", line 7, in chat\n",
      "    response = qa_chain({\"question\": message, \"chat_history\": chat_history})\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 182, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain\\chains\\base.py\", line 389, in __call__\n",
      "    return self.invoke(\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain\\chains\\base.py\", line 170, in invoke\n",
      "    raise e\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain\\chains\\base.py\", line 160, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain\\chains\\conversational_retrieval\\base.py\", line 170, in _call\n",
      "    answer = self.combine_docs_chain.run(\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 182, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain\\chains\\base.py\", line 611, in run\n",
      "    return self(kwargs, callbacks=callbacks, tags=tags, metadata=metadata)[\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 182, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain\\chains\\base.py\", line 389, in __call__\n",
      "    return self.invoke(\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain\\chains\\base.py\", line 170, in invoke\n",
      "    raise e\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain\\chains\\base.py\", line 160, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain\\chains\\combine_documents\\base.py\", line 138, in _call\n",
      "    output, extra_return_dict = self.combine_docs(\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain\\chains\\combine_documents\\stuff.py\", line 259, in combine_docs\n",
      "    return self.llm_chain.predict(callbacks=callbacks, **inputs), {}\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain\\chains\\llm.py\", line 318, in predict\n",
      "    return self(kwargs, callbacks=callbacks)[self.output_key]\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 182, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain\\chains\\base.py\", line 389, in __call__\n",
      "    return self.invoke(\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain\\chains\\base.py\", line 170, in invoke\n",
      "    raise e\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain\\chains\\base.py\", line 160, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain\\chains\\llm.py\", line 126, in _call\n",
      "    response = self.generate([inputs], run_manager=run_manager)\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain\\chains\\llm.py\", line 138, in generate\n",
      "    return self.llm.generate_prompt(\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 786, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 643, in generate\n",
      "    raise e\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 633, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 851, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain_community\\chat_models\\ollama.py\", line 291, in _generate\n",
      "    final_chunk = self._chat_stream_with_aggregation(\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain_community\\chat_models\\ollama.py\", line 222, in _chat_stream_with_aggregation\n",
      "    for stream_resp in self._create_chat_stream(messages, stop, **kwargs):\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain_community\\chat_models\\ollama.py\", line 194, in _create_chat_stream\n",
      "    yield from self._create_stream(\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain_community\\llms\\ollama.py\", line 273, in _create_stream\n",
      "    raise ValueError(\n",
      "ValueError: Ollama call failed with status code 500. Details: {\"error\":\"model requires more system memory (7.7 GiB) than is available (3.0 GiB)\"}\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\gradio\\queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\gradio\\blocks.py\", line 2047, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\gradio\\blocks.py\", line 1592, in call_function\n",
      "    prediction = await fn(*processed_input)\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\gradio\\utils.py\", line 836, in async_wrapper\n",
      "    response = await f(*args, **kwargs)\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\gradio\\chat_interface.py\", line 618, in _submit_fn\n",
      "    response = await anyio.to_thread.run_sync(\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2505, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 1005, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"C:\\Users\\human-06\\AppData\\Local\\Temp\\ipykernel_10352\\4160252103.py\", line 7, in chat\n",
      "    response = qa_chain({\"question\": message, \"chat_history\": chat_history})\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 182, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain\\chains\\base.py\", line 389, in __call__\n",
      "    return self.invoke(\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain\\chains\\base.py\", line 170, in invoke\n",
      "    raise e\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain\\chains\\base.py\", line 160, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain\\chains\\conversational_retrieval\\base.py\", line 170, in _call\n",
      "    answer = self.combine_docs_chain.run(\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 182, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain\\chains\\base.py\", line 611, in run\n",
      "    return self(kwargs, callbacks=callbacks, tags=tags, metadata=metadata)[\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 182, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain\\chains\\base.py\", line 389, in __call__\n",
      "    return self.invoke(\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain\\chains\\base.py\", line 170, in invoke\n",
      "    raise e\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain\\chains\\base.py\", line 160, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain\\chains\\combine_documents\\base.py\", line 138, in _call\n",
      "    output, extra_return_dict = self.combine_docs(\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain\\chains\\combine_documents\\stuff.py\", line 259, in combine_docs\n",
      "    return self.llm_chain.predict(callbacks=callbacks, **inputs), {}\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain\\chains\\llm.py\", line 318, in predict\n",
      "    return self(kwargs, callbacks=callbacks)[self.output_key]\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 182, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain\\chains\\base.py\", line 389, in __call__\n",
      "    return self.invoke(\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain\\chains\\base.py\", line 170, in invoke\n",
      "    raise e\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain\\chains\\base.py\", line 160, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain\\chains\\llm.py\", line 126, in _call\n",
      "    response = self.generate([inputs], run_manager=run_manager)\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain\\chains\\llm.py\", line 138, in generate\n",
      "    return self.llm.generate_prompt(\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 786, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 643, in generate\n",
      "    raise e\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 633, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 851, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain_community\\chat_models\\ollama.py\", line 291, in _generate\n",
      "    final_chunk = self._chat_stream_with_aggregation(\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain_community\\chat_models\\ollama.py\", line 222, in _chat_stream_with_aggregation\n",
      "    for stream_resp in self._create_chat_stream(messages, stop, **kwargs):\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain_community\\chat_models\\ollama.py\", line 194, in _create_chat_stream\n",
      "    yield from self._create_stream(\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain_community\\llms\\ollama.py\", line 273, in _create_stream\n",
      "    raise ValueError(\n",
      "ValueError: Ollama call failed with status code 500. Details: {\"error\":\"model requires more system memory (7.7 GiB) than is available (3.4 GiB)\"}\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\gradio\\queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\gradio\\blocks.py\", line 2047, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\gradio\\blocks.py\", line 1592, in call_function\n",
      "    prediction = await fn(*processed_input)\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\gradio\\utils.py\", line 836, in async_wrapper\n",
      "    response = await f(*args, **kwargs)\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\gradio\\chat_interface.py\", line 618, in _submit_fn\n",
      "    response = await anyio.to_thread.run_sync(\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2505, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 1005, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"C:\\Users\\human-06\\AppData\\Local\\Temp\\ipykernel_10352\\4160252103.py\", line 7, in chat\n",
      "    response = qa_chain({\"question\": message, \"chat_history\": chat_history})\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 182, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain\\chains\\base.py\", line 389, in __call__\n",
      "    return self.invoke(\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain\\chains\\base.py\", line 170, in invoke\n",
      "    raise e\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain\\chains\\base.py\", line 160, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain\\chains\\conversational_retrieval\\base.py\", line 170, in _call\n",
      "    answer = self.combine_docs_chain.run(\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 182, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain\\chains\\base.py\", line 611, in run\n",
      "    return self(kwargs, callbacks=callbacks, tags=tags, metadata=metadata)[\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 182, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain\\chains\\base.py\", line 389, in __call__\n",
      "    return self.invoke(\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain\\chains\\base.py\", line 170, in invoke\n",
      "    raise e\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain\\chains\\base.py\", line 160, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain\\chains\\combine_documents\\base.py\", line 138, in _call\n",
      "    output, extra_return_dict = self.combine_docs(\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain\\chains\\combine_documents\\stuff.py\", line 259, in combine_docs\n",
      "    return self.llm_chain.predict(callbacks=callbacks, **inputs), {}\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain\\chains\\llm.py\", line 318, in predict\n",
      "    return self(kwargs, callbacks=callbacks)[self.output_key]\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 182, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain\\chains\\base.py\", line 389, in __call__\n",
      "    return self.invoke(\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain\\chains\\base.py\", line 170, in invoke\n",
      "    raise e\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain\\chains\\base.py\", line 160, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain\\chains\\llm.py\", line 126, in _call\n",
      "    response = self.generate([inputs], run_manager=run_manager)\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain\\chains\\llm.py\", line 138, in generate\n",
      "    return self.llm.generate_prompt(\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 786, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 643, in generate\n",
      "    raise e\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 633, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 851, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain_community\\chat_models\\ollama.py\", line 291, in _generate\n",
      "    final_chunk = self._chat_stream_with_aggregation(\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain_community\\chat_models\\ollama.py\", line 222, in _chat_stream_with_aggregation\n",
      "    for stream_resp in self._create_chat_stream(messages, stop, **kwargs):\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain_community\\chat_models\\ollama.py\", line 194, in _create_chat_stream\n",
      "    yield from self._create_stream(\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain_community\\llms\\ollama.py\", line 273, in _create_stream\n",
      "    raise ValueError(\n",
      "ValueError: Ollama call failed with status code 500. Details: {\"error\":\"model requires more system memory (7.7 GiB) than is available (3.3 GiB)\"}\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\gradio\\queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\gradio\\blocks.py\", line 2047, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\gradio\\blocks.py\", line 1592, in call_function\n",
      "    prediction = await fn(*processed_input)\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\gradio\\utils.py\", line 836, in async_wrapper\n",
      "    response = await f(*args, **kwargs)\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\gradio\\chat_interface.py\", line 618, in _submit_fn\n",
      "    response = await anyio.to_thread.run_sync(\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2505, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 1005, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"C:\\Users\\human-06\\AppData\\Local\\Temp\\ipykernel_10352\\4160252103.py\", line 7, in chat\n",
      "    response = qa_chain({\"question\": message, \"chat_history\": chat_history})\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 182, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain\\chains\\base.py\", line 389, in __call__\n",
      "    return self.invoke(\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain\\chains\\base.py\", line 170, in invoke\n",
      "    raise e\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain\\chains\\base.py\", line 160, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain\\chains\\conversational_retrieval\\base.py\", line 170, in _call\n",
      "    answer = self.combine_docs_chain.run(\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 182, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain\\chains\\base.py\", line 611, in run\n",
      "    return self(kwargs, callbacks=callbacks, tags=tags, metadata=metadata)[\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 182, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain\\chains\\base.py\", line 389, in __call__\n",
      "    return self.invoke(\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain\\chains\\base.py\", line 170, in invoke\n",
      "    raise e\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain\\chains\\base.py\", line 160, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain\\chains\\combine_documents\\base.py\", line 138, in _call\n",
      "    output, extra_return_dict = self.combine_docs(\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain\\chains\\combine_documents\\stuff.py\", line 259, in combine_docs\n",
      "    return self.llm_chain.predict(callbacks=callbacks, **inputs), {}\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain\\chains\\llm.py\", line 318, in predict\n",
      "    return self(kwargs, callbacks=callbacks)[self.output_key]\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py\", line 182, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain\\chains\\base.py\", line 389, in __call__\n",
      "    return self.invoke(\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain\\chains\\base.py\", line 170, in invoke\n",
      "    raise e\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain\\chains\\base.py\", line 160, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain\\chains\\llm.py\", line 126, in _call\n",
      "    response = self.generate([inputs], run_manager=run_manager)\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain\\chains\\llm.py\", line 138, in generate\n",
      "    return self.llm.generate_prompt(\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 786, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 643, in generate\n",
      "    raise e\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 633, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 851, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain_community\\chat_models\\ollama.py\", line 291, in _generate\n",
      "    final_chunk = self._chat_stream_with_aggregation(\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain_community\\chat_models\\ollama.py\", line 222, in _chat_stream_with_aggregation\n",
      "    for stream_resp in self._create_chat_stream(messages, stop, **kwargs):\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain_community\\chat_models\\ollama.py\", line 194, in _create_chat_stream\n",
      "    yield from self._create_stream(\n",
      "  File \"c:\\AiProject\\.venv\\lib\\site-packages\\langchain_community\\llms\\ollama.py\", line 273, in _create_stream\n",
      "    raise ValueError(\n",
      "ValueError: Ollama call failed with status code 500. Details: {\"error\":\"model requires more system memory (7.7 GiB) than is available (3.3 GiB)\"}\n"
     ]
    }
   ],
   "source": [
    "# 서버 실행\n",
    "demo.launch(server_port=7861, server_name=\"0.0.0.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "hbb6ddi9Dp7W"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7861\n"
     ]
    }
   ],
   "source": [
    "demo.close()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMJrHa4jasbRF3jGvScJH0L",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
